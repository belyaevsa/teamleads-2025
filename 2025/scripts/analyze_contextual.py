#!/usr/bin/env python3
"""
Contextual Topic Analysis - Analyzes messages with surrounding context
Identifies daily topics by looking at conversation threads and message clusters
"""

import csv
import re
from collections import defaultdict, Counter
from datetime import datetime

# Load messages
print("Loading messages...")
messages = []
with open('../data/messages_export.csv', 'r', encoding='utf-8') as f:
    reader = csv.DictReader(f)
    for row in reader:
        # Parse timestamp
        row['timestamp'] = datetime.strptime(row['created_at'], '%Y-%m-%d %H:%M:%S')
        row['date'] = row['timestamp'].date()
        messages.append(row)

print(f"Loaded {len(messages)} messages\n")

# Sort by timestamp for sequential analysis
messages.sort(key=lambda x: x['timestamp'])

# Build conversation threads
print("Building conversation threads...")
threads = defaultdict(list)  # reply_to_messageid -> list of messages
message_by_id = {}  # messageid -> message

for msg in messages:
    msg_id = msg.get('messageid', '')
    reply_to = msg.get('reply_to_messageid', '')

    message_by_id[msg_id] = msg

    if reply_to:
        threads[reply_to].append(msg)

print(f"Found {len(threads)} conversation threads\n")

# Topic detection keywords (expanded and contextual)
TOPIC_PATTERNS = {
    '–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥–æ–π': [
        '–∫–æ–º–∞–Ω–¥', '—Ç–∏–º–ª–∏–¥', '—É–ø—Ä–∞–≤–ª–µ–Ω', '—Ä—É–∫–æ–≤–æ–¥—Å—Ç', '–ª–∏–¥–µ—Ä—Å—Ç',
        '–ø–æ–¥—á–∏–Ω–µ–Ω', '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫', '–º–æ—Ç–∏–≤–∞—Ü', '–¥–µ–ª–µ–≥–∏—Ä', '–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç',
        '–æ–¥–∏–Ω –Ω–∞ –æ–¥–∏–Ω', '1-1', 'one-on-one', '—Ñ–∏–¥–±–µ–∫', '–æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å'
    ],
    '–ü—Ä–æ—Ü–µ—Å—Å—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏': [
        '–ø—Ä–æ—Ü–µ—Å—Å', '–º–µ—Ç–æ–¥–æ–ª–æ–≥', 'scrum', 'agile', 'kanban', '—Å–ø—Ä–∏–Ω—Ç',
        '—Ä–µ—Ç—Ä–æ—Å–ø–µ–∫—Ç–∏–≤', '–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω', '–æ—Ü–µ–Ω–∫', '—Å—Ç–µ–Ω–¥–∞–ø', 'daily',
        '—Ä–µ–ª–∏–∑', '–¥–µ–ø–ª–æ–π', 'ci/cd', 'code review', '—Ä–µ–≤—å—é –∫–æ–¥–∞'
    ],
    '–ù–∞–π–º –∏ HR': [
        '–Ω–∞–π–º', '–∫–∞–Ω–¥–∏–¥–∞—Ç', '—Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω', '–∏–Ω—Ç–µ—Ä–≤—å—é', '—Ä–µ–∑—é–º–µ',
        '–∑–∞—Ä–ø–ª–∞—Ç', '–æ—Ñ—Ñ–µ—Ä', '–æ–Ω–±–æ—Ä–¥–∏–Ω–≥', '—É–≤–æ–ª—å–Ω–µ–Ω', '–∏—Å–ø—ã—Ç–∞—Ç–µ–ª',
        '–¥–∂—É–Ω', '–º–∏–¥–ª', '—Å–µ–Ω—å–æ—Ä', '–≥—Ä–µ–π–¥', '–∫–∞—Ä—å–µ—Ä'
    ],
    '–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞': [
        '–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä', '–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω', '–º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å', '–º–æ–Ω–æ–ª–∏—Ç',
        '–ø–∞—Ç—Ç–µ—Ä–Ω', 'design', '–º–∞—Å—à—Ç–∞–±', '—Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥', '—Ç–µ—Ö–¥–æ–ª–≥',
        'legacy', '–º–∏–≥—Ä–∞—Ü', '–∏–Ω—Ç–µ–≥—Ä–∞—Ü'
    ],
    '–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ': [
        '—Ç–µ—Å—Ç', 'qa', '–±–∞–≥', 'bug', '–∞–≤—Ç–æ—Ç–µ—Å—Ç', 'unit', 'integration',
        'e2e', 'regression', 'coverage', '–ø–æ–∫—Ä—ã—Ç–∏', '–∫–∞—á–µ—Å—Ç–≤–æ'
    ],
    'AI/ML': [
        'ai', 'ml', 'chatgpt', 'gpt', 'llm', '–Ω–µ–π—Ä–æ–Ω', '–º–∞—à–∏–Ω–Ω –æ–±—É—á–µ–Ω',
        'copilot', '–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤', '–ø—Ä–æ–º–ø—Ç', '–º–æ–¥–µ–ª—å'
    ],
    '–ö–æ–Ω—Ñ–ª–∏–∫—Ç—ã –∏ –ø—Ä–æ–±–ª–µ–º—ã': [
        '–∫–æ–Ω—Ñ–ª–∏–∫—Ç', '—Ä–∞–∑–Ω–æ–≥–ª–∞—Å–∏', '—Å–ø–æ—Ä', '–ø—Ä–æ–±–ª–µ–º', '—Å–ª–æ–∂–Ω–æ—Å—Ç',
        '–Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è', '–∑–∞—Å—Ç—Ä—è–ª', '–Ω–µ –ø–æ–Ω–∏–º–∞—é', '–≤—ã–≥–æ—Ä–∞–Ω', 'burnout'
    ],
    '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Ä–æ—Å—Ç': [
        '–æ–±—É—á–µ–Ω', '–∫—É—Ä—Å', '–∫–Ω–∏–≥', '—Å—Ç–∞—Ç—å—è', '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü', '–º–∏—Ç–∞–ø',
        '–º–µ–Ω—Ç–æ—Ä–∏–Ω–≥', '—Ä–∞–∑–≤–∏—Ç–∏', '–ø—Ä–æ–∫–∞—á–∫', '–Ω–∞–≤—ã–∫', '–∫–æ–º–ø–µ—Ç–µ–Ω—Ü'
    ],
    '–ú–µ—Ç—Ä–∏–∫–∏ –∏ KPI': [
        '–º–µ—Ç—Ä–∏–∫', 'kpi', 'okr', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω', 'velocity',
        '—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç', '–∏–∑–º–µ—Ä–µ–Ω', '–∞–Ω–∞–ª–∏–∑', 'dashboard', '–¥–∞—à–±–æ—Ä–¥'
    ],
    '–£–¥–∞–ª–µ–Ω–∫–∞ –∏ –æ—Ñ–∏—Å': [
        '—É–¥–∞–ª–µ–Ω–∫', 'remote', '–æ—Ñ–∏—Å', '–≥–∏–±—Ä–∏–¥', '—Ä–µ–ª–æ–∫–∞—Ü',
        'timezone', '—á–∞—Å–æ–≤–æ–π –ø–æ—è—Å', '—Å–æ–∑–≤–æ–Ω', '–≤—Å—Ç—Ä–µ—á'
    ]
}

def get_message_context(msg, messages, window=5):
    """Get surrounding messages for context (within same day)"""
    msg_idx = messages.index(msg)
    msg_date = msg['date']

    context_before = []
    context_after = []

    # Look back
    for i in range(max(0, msg_idx - window), msg_idx):
        if messages[i]['date'] == msg_date:
            context_before.append(messages[i])

    # Look ahead
    for i in range(msg_idx + 1, min(len(messages), msg_idx + window + 1)):
        if messages[i]['date'] == msg_date:
            context_after.append(messages[i])

    return context_before, context_after

def get_thread_context(msg, threads, message_by_id):
    """Get all messages in the conversation thread"""
    thread_messages = []

    # Get parent message if this is a reply
    reply_to = msg.get('reply_to_messageid', '')
    if reply_to and reply_to in message_by_id:
        thread_messages.append(message_by_id[reply_to])

    # Get replies to this message
    msg_id = msg.get('messageid', '')
    if msg_id in threads:
        thread_messages.extend(threads[msg_id])

    return thread_messages

def analyze_topic_with_context(msg, context_before, context_after, thread_context):
    """Analyze message topic considering all context"""
    # Combine all text for analysis
    msg_text = msg.get('user_message', '').lower()

    # Context text
    context_text = ' '.join([
        m.get('user_message', '').lower()
        for m in context_before + context_after + thread_context
        if m.get('user_message')
    ])

    # Combined text with more weight on the message itself
    combined = msg_text + ' ' + msg_text + ' ' + context_text

    # Score topics
    topic_scores = {}
    for topic, keywords in TOPIC_PATTERNS.items():
        score = 0
        for keyword in keywords:
            # Count in message (weight: 3)
            score += msg_text.count(keyword) * 3
            # Count in context (weight: 1)
            score += context_text.count(keyword)

        if score > 0:
            topic_scores[topic] = score

    return topic_scores

def identify_message_topics(msg, context_before, context_after, thread_context, min_score=2):
    """Identify topics for a message based on context"""
    topic_scores = analyze_topic_with_context(msg, context_before, context_after, thread_context)

    # Return topics that meet minimum score
    topics = [topic for topic, score in topic_scores.items() if score >= min_score]

    # If no specific topic, mark as General
    return topics if topics else ['–û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã']

# Analyze all messages with context
print("Analyzing messages with contextual understanding...")
daily_topics = defaultdict(lambda: defaultdict(list))  # date -> topic -> messages
message_topics = {}  # messageid -> topics list

for i, msg in enumerate(messages):
    if i % 1000 == 0:
        print(f"Processed {i}/{len(messages)} messages...")

    # Get context
    context_before, context_after = get_message_context(msg, messages)
    thread_context = get_thread_context(msg, threads, message_by_id)

    # Identify topics
    topics = identify_message_topics(msg, context_before, context_after, thread_context)

    msg_id = msg.get('messageid', '')
    msg_date = msg['date']

    message_topics[msg_id] = topics

    for topic in topics:
        daily_topics[msg_date][topic].append(msg)

print(f"\nCompleted analysis of {len(messages)} messages\n")

# Generate daily topic report
print("=" * 80)
print("DAILY TOPIC SUMMARY - 2025")
print("=" * 80)

# Get all dates sorted
all_dates = sorted(daily_topics.keys())

for date in all_dates:
    topics = daily_topics[date]
    total_msgs = sum(len(msgs) for msgs in topics.values())

    print(f"\nüìÖ {date.strftime('%Y-%m-%d (%A)')}")
    print(f"   –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {total_msgs}")
    print(f"   –¢–µ–º—ã –¥–Ω—è:")

    # Sort topics by number of messages
    sorted_topics = sorted(topics.items(), key=lambda x: len(x[1]), reverse=True)

    for topic, msgs in sorted_topics[:5]:  # Top 5 topics per day
        pct = (len(msgs) / total_msgs * 100)
        print(f"      ‚Ä¢ {topic}: {len(msgs)} —Å–æ–æ–±—â–µ–Ω–∏–π ({pct:.1f}%)")

# Monthly topic aggregation
print("\n\n" + "=" * 80)
print("TOPIC TRENDS BY MONTH")
print("=" * 80)

monthly_topics = defaultdict(Counter)  # month -> topic -> count

for date, topics in daily_topics.items():
    month = date.strftime('%Y-%m')
    for topic, msgs in topics.items():
        monthly_topics[month][topic] += len(msgs)

for month in sorted(monthly_topics.keys()):
    topics = monthly_topics[month]
    total = sum(topics.values())

    print(f"\n{month}:")
    print(f"  –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π —Å —Ç–µ–º–∞–º–∏: {total}")
    print(f"  –¢–æ–ø-5 —Ç–µ–º:")

    for topic, count in topics.most_common(5):
        pct = (count / total * 100)
        print(f"    {topic:30s}: {count:5d} ({pct:5.1f}%)")

# Management and Process specific analysis
print("\n\n" + "=" * 80)
print("–£–ü–†–ê–í–õ–ï–ù–ò–ï –ò –ü–†–û–¶–ï–°–°–´ - –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó")
print("=" * 80)

management_topics = ['–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥–æ–π', '–ü—Ä–æ—Ü–µ—Å—Å—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏']

for topic in management_topics:
    print(f"\n{'=' * 80}")
    print(f"–¢–µ–º–∞: {topic}")
    print('=' * 80)

    # Days with discussions
    topic_days = []
    for date, topics in daily_topics.items():
        if topic in topics and len(topics[topic]) >= 3:  # At least 3 messages
            topic_days.append((date, topics[topic]))

    topic_days.sort(key=lambda x: len(x[1]), reverse=True)

    print(f"\n–í—Å–µ–≥–æ –¥–Ω–µ–π —Å –æ–±—Å—É–∂–¥–µ–Ω–∏—è–º–∏: {len(topic_days)}")
    print(f"\n–¢–æ–ø-10 —Å–∞–º—ã—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –¥–Ω–µ–π:\n")

    for date, msgs in topic_days[:10]:
        print(f"  {date.strftime('%Y-%m-%d')}: {len(msgs)} —Å–æ–æ–±—â–µ–Ω–∏–π")

        # Show snippet from most representative message
        msg_with_context = msgs[0]
        snippet = msg_with_context.get('user_message', '')[:150]
        if len(snippet) > 0:
            snippet = snippet.replace('\n', ' ').strip()
            print(f"    ‚îî‚îÄ ¬´{snippet}...¬ª")

# Overall topic distribution
print("\n\n" + "=" * 80)
print("–û–ë–©–ï–ï –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –¢–ï–ú –ó–ê –ì–û–î")
print("=" * 80)

overall_topics = Counter()
for topics_list in message_topics.values():
    for topic in topics_list:
        overall_topics[topic] += 1

total_topic_mentions = sum(overall_topics.values())

print(f"\n–í—Å–µ–≥–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π —Ç–µ–º: {total_topic_mentions}")
print(f"(–æ–¥–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –º–æ–∂–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç—å—Å—è –∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º —Ç–µ–º–∞–º)\n")

for topic, count in overall_topics.most_common():
    pct = (count / len(messages)) * 100
    print(f"{topic:30s}: {count:6d} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π ({pct:5.1f}% –æ—Ç –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏–π)")

# Save detailed results
print("\n\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")

# Daily topics CSV
with open('../data/daily_topics.csv', 'w', encoding='utf-8') as f:
    f.write('date,topic,message_count\n')
    for date in sorted(daily_topics.keys()):
        for topic, msgs in daily_topics[date].items():
            f.write(f"{date},{topic},{len(msgs)}\n")

# Message topics mapping
with open('../data/message_topics.csv', 'w', encoding='utf-8') as f:
    f.write('messageid,topics\n')
    for msg_id, topics in message_topics.items():
        topics_str = '|'.join(topics)
        f.write(f"{msg_id},{topics_str}\n")

# Management days detailed
with open('../data/management_days.csv', 'w', encoding='utf-8') as f:
    f.write('date,topic,message_count,sample_message\n')
    for topic in management_topics:
        for date, topics in daily_topics.items():
            if topic in topics:
                msgs = topics[topic]
                sample = msgs[0].get('user_message', '').replace('\n', ' ')[:200]
                f.write(f"{date},{topic},{len(msgs)},\"{sample}\"\n")

print("\n‚úÖ –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
print("\n–§–∞–π–ª—ã —Å–æ–∑–¥–∞–Ω—ã:")
print("  - daily_topics.csv (—Ç–µ–º—ã –ø–æ –¥–Ω—è–º)")
print("  - message_topics.csv (—Ç–µ–º—ã –∫–∞–∂–¥–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è)")
print("  - management_days.csv (–¥–Ω–∏ —Å –æ–±—Å—É–∂–¥–µ–Ω–∏–µ–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è/–ø—Ä–æ—Ü–µ—Å—Å–æ–≤)")
